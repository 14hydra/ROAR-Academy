{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0937657",
   "metadata": {},
   "source": [
    "# Reinforcement Learning and Deep Q-Network (DQN)\n",
    "#### ROAR Academy\n",
    "**Author:** Allen Y. Yang, PhD\n",
    "**Adapted for Jupyter Notebook by:** ROAR Academy Team\n",
    "(c) Copyright, Intelligent Racing Inc., 2020.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42058fe2",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Reinforcement Learning (RL) is a powerful machine learning paradigm inspired by how agents learn to act in an environment through trial and error. In this notebook, we explore the core concepts of RL, the Bellman equation, and Deep Q-Networks (DQN). We will train an agent using DQN on the classic CartPole-v1 environment from OpenAI's `gym`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0ba712",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Basics\n",
    "An RL problem is typically modeled as a **Markov Decision Process (MDP)**. The agent interacts with an environment over discrete time steps.\n",
    "\n",
    "- **State (s)**: Current situation of the agent.\n",
    "- **Action (a)**: Choices available to the agent.\n",
    "- **Reward (r)**: Feedback from the environment.\n",
    "- **Policy (π)**: Strategy used by the agent.\n",
    "- **Q-Value**: Expected future rewards given a state and action.\n",
    "\n",
    "### Bellman Equation\n",
    "The Bellman Equation provides a recursive decomposition for computing Q-values:\n",
    "\n",
    "$$Q(s_t, a) = r(s_t, a) + \\gamma \\max_{a'} Q(s_{t+1}, a')$$\n",
    "Where \\( \\gamma \\) is the discount factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627721b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "import warnings\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "EPISODES = 100\n",
    "RENDER_EVERY = 10  # Render every N episodes to see progress\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95  # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state, verbose=0)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.gamma * np.amax(self.model.predict(next_state, verbose=0)[0])\n",
    "            target_f = self.model.predict(state, verbose=0)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n",
    "\n",
    "# Try different ways to create the environment\n",
    "env = None\n",
    "render_method = None\n",
    "\n",
    "# Method 1: Try with old API\n",
    "try:\n",
    "    env = gym.make('CartPole-v1')\n",
    "    env.reset()\n",
    "    env.render(mode='rgb_array')\n",
    "    render_method = 'old_rgb'\n",
    "    print(\"Using old render API with rgb_array mode\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Method 2: Try default render\n",
    "if env is None or render_method is None:\n",
    "    try:\n",
    "        env = gym.make('CartPole-v1')\n",
    "        env.reset()\n",
    "        env.render()\n",
    "        render_method = 'old_default'\n",
    "        print(\"Using old render API with default mode\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Method 3: Just create environment without render\n",
    "if env is None:\n",
    "    env = gym.make('CartPole-v1')\n",
    "    render_method = 'manual'\n",
    "    print(\"Created environment, will attempt manual visualization\")\n",
    "\n",
    "# Test reset to check API version\n",
    "state_or_tuple = env.reset()\n",
    "if isinstance(state_or_tuple, tuple):\n",
    "    state, _ = state_or_tuple\n",
    "    uses_new_api = True\n",
    "else:\n",
    "    state = state_or_tuple\n",
    "    uses_new_api = False\n",
    "\n",
    "print(f\"API version: {'new' if uses_new_api else 'old'}\")\n",
    "\n",
    "# Function to get frame based on render method\n",
    "def get_frame(env, render_method):\n",
    "    try:\n",
    "        if render_method == 'old_rgb':\n",
    "            return env.render(mode='rgb_array')\n",
    "        elif render_method == 'old_default':\n",
    "            # Try to get rgb_array even if not default\n",
    "            try:\n",
    "                return env.render(mode='rgb_array')\n",
    "            except:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Manual visualization fallback\n",
    "def draw_cartpole_state(state, step, reward):\n",
    "    \"\"\"Manually draw the cartpole state\"\"\"\n",
    "    cart_pos = state[0]\n",
    "    pole_angle = state[2]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    # Set up the plot\n",
    "    ax.set_xlim(-3, 3)\n",
    "    ax.set_ylim(-1, 2)\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "    # Draw track\n",
    "    ax.plot([-2.4, 2.4], [0, 0], 'k-', linewidth=2)\n",
    "    ax.plot([-2.4, -2.4], [-0.05, 0.05], 'r-', linewidth=4)\n",
    "    ax.plot([2.4, 2.4], [-0.05, 0.05], 'r-', linewidth=4)\n",
    "    \n",
    "    # Draw cart\n",
    "    cart_width = 0.3\n",
    "    cart_height = 0.2\n",
    "    cart = plt.Rectangle((cart_pos - cart_width/2, -cart_height/2), \n",
    "                        cart_width, cart_height, \n",
    "                        fill=True, color='blue')\n",
    "    ax.add_patch(cart)\n",
    "    \n",
    "    # Draw pole\n",
    "    pole_length = 1.0\n",
    "    pole_end_x = cart_pos + pole_length * np.sin(pole_angle)\n",
    "    pole_end_y = pole_length * np.cos(pole_angle)\n",
    "    ax.plot([cart_pos, pole_end_x], [0, pole_end_y], 'brown', linewidth=8)\n",
    "    \n",
    "    # Add pole joint\n",
    "    circle = plt.Circle((cart_pos, 0), 0.05, color='black')\n",
    "    ax.add_patch(circle)\n",
    "    \n",
    "    # Add title\n",
    "    ax.set_title(f'Step: {step}, Reward: {reward:.0f}, Angle: {np.degrees(pole_angle):.1f}°')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlabel('Position')\n",
    "    ax.set_ylabel('Height')\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Run one episode\n",
    "angle = state[2]\n",
    "angle_velocity = state[3]\n",
    "integral = 0.0\n",
    "prev_error = angle\n",
    "total_reward = 0\n",
    "batch_size = 32\n",
    "\n",
    "print(\"\\nRunning CartPole with PID Controller...\")\n",
    "print(\"Watch the game below:\\n\")\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "for t in range(EPISODES):\n",
    "    # DQN control\n",
    "    action = agent.act(state)\n",
    "    \n",
    "    # Step environment\n",
    "    if uses_new_api:\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "    else:\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "    reward = reward if not done else -10\n",
    "    next_state = np.reshape(next_state, [1, state_size])\n",
    "    agent.remember(state, action, reward, next_state, done)\n",
    "\n",
    "     # Train the agent\n",
    "    if len(agent.memory) > batch_size:\n",
    "        agent.replay(batch_size)\n",
    "    \n",
    "    # Update state\n",
    "    angle = state[2]\n",
    "    angle_velocity = state[3]\n",
    "    total_reward += reward\n",
    "    \n",
    "    # Display every 10 steps or when done\n",
    "    if t % RENDER_EVERY == 0 or done:\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        # Try to get frame\n",
    "        frame = get_frame(env, render_method)\n",
    "        \n",
    "        if frame is not None:\n",
    "            # Display captured frame\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.imshow(frame)\n",
    "            plt.axis('off')\n",
    "            plt.title(f'Step: {t+1}, Reward: {total_reward:.0f}')\n",
    "            plt.show()\n",
    "        else:\n",
    "            # Use manual visualization\n",
    "            fig = draw_cartpole_state(state, t+1, total_reward)\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "    \n",
    "    if done:\n",
    "        print(f\"\\nGame Over! Lasted {t + 1} steps with total reward: {total_reward}\")\n",
    "        \n",
    "        # Show final state\n",
    "        fig = draw_cartpole_state(state, t+1, total_reward)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        break\n",
    "else:\n",
    "    print(f\"\\nCompleted all 200 steps! Total reward: {total_reward}\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Training Complete!\")\n",
    "print(f\"Final average score (last 10 episodes): {np.mean(scores[-10:]):.1f}\")\n",
    "print(f\"Best score achieved: {max(scores)}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Optional: Plot learning curve\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(scores, alpha=0.6, label='Episode scores')\n",
    "    \n",
    "    # Calculate rolling average\n",
    "    window = 10\n",
    "    rolling_avg = [np.mean(scores[max(0, i-window+1):i+1]) for i in range(len(scores))]\n",
    "    plt.plot(rolling_avg, linewidth=2, label=f'{window}-episode average')\n",
    "    \n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('DQN Learning Progress on CartPole-v1')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "except ImportError:\n",
    "    print(\"Matplotlib not available for plotting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d820e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "batch_size = 32\n",
    "episodes = 10  # Reduce for display purposes\n",
    "\n",
    "for e in range(episodes):\n",
    "    state, _ = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    frames = [env.render()]  # Initial frame\n",
    "    for time_t in range(500):\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, truncated, _ = env.step(action)\n",
    "        reward = reward if not done else -10\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        frames.append(env.render())  # Frame after action\n",
    "        if done or truncated:\n",
    "            break\n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.replay(batch_size)\n",
    "    print(f\"episode: {e+1}/{episodes}, score: {time_t + 1}, epsilon: {agent.epsilon:.2f}\")\n",
    "    display_cartpole_frames(frames)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2712f10",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this notebook, we explored the fundamental concepts of reinforcement learning and implemented a Deep Q-Network (DQN) using TensorFlow and OpenAI Gym. With just a few episodes, the agent can learn to balance the pole effectively. Experiment with the parameters and environment to deepen your understanding!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "roar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
